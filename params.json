{"name":"Practical Machine Learning Project","tagline":"John Doe, October 26, 2014","body":"\r\nThis project aims to create an algorithm to analyze whether data from devices with accelerometers can correctly predict the exact movement the person wearing is doing. \r\n\r\n## Background: Are they performing barbell lifts correctly?\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.\r\n\r\n## Data\r\n### Getting Data\r\nThe data for this project come from [here](http://groupware.les.inf.puc-rio.br/har). The dataset used in training and cross-validation is stored in `totalRaw`, and the other small dataset used to predict values submitted to Coursera is stored in `testRaw`. When loading data, any empty values or those with \"NA\" or \"#DIV/0!\" would be treated as `NA`. We have 19622 observations of 160 features in `totalRaw`.\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(lattice)\r\nlibrary(kernlab)\r\nlibrary(randomForest)\r\n```\r\n\r\n```\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n\r\n```r\r\nif (!file.exists(\"pml-training.csv\")) {\r\n  download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", destfile = \"pml-training.csv\")\r\n}\r\nif (!file.exists(\"pml-testing.csv\")) {\r\n  download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", destfile = \"pml-testing.csv\")\r\n}\r\n\r\ntotalRaw <- read.csv(\"pml-training.csv\", sep = \",\", na.strings = c(\"\", \"NA\", \"#DIV/0!\"))\r\ntestRaw <- read.csv(\"pml-testing.csv\", sep = \",\", na.strings = c(\"\", \"NA\", \"#DIV/0!\"))\r\n```\r\n\r\n## Data Cleaning\r\nFirst exclude all those columns with `NA` values, as those insignificant features would interfere with algorithm's accuracy and efficiency. Doing so would eliminate 100 variables. \r\nThen we remove columns corresponding to descriptive contents, such as `user_name`, and anything related to `timestamp`. These are stored in the first seven columns in the data set.\r\n\r\n\r\n```r\r\ntotalRaw <- totalRaw[,colSums(is.na(totalRaw))==0]\r\ntotalRaw <- totalRaw[,-1:-7]\r\n```\r\n\r\n## Data Partition\r\nNext, we partition data to have 70% into training and 30% into testing in order to have good cross-validation.\r\n\r\n\r\n```r\r\ninTrain <- createDataPartition(y=totalRaw$classe, p = 0.7, list = FALSE)\r\ntraining <- totalRaw[inTrain,]\r\ntesting <- totalRaw[-inTrain,]\r\n```\r\n\r\n# Model Building\r\nWe utilize `randomForest` model to build the classification tree, since it has highest accuracy with minimum overfitting issues. During training, cross-validation (`cv`) has been chosen to control the process.\r\n\r\n\r\n```r\r\n# Register for parallel computing\r\nlibrary(doParallel);\r\n```\r\n\r\n```\r\n## Loading required package: foreach\r\n## Loading required package: iterators\r\n## Loading required package: parallel\r\n```\r\n\r\n```r\r\nrCluster <- makePSOCKcluster(4);\r\nregisterDoParallel(rCluster);\r\n\r\n# Command to train machine\r\ntrControl = trainControl(method = \"cv\", number = 4, allowParallel =TRUE);\r\nrffit <- train(training$classe ~.,data = training,method=\"rf\",trControl=trControl);\r\n```\r\n\r\n# Cross-validation\r\nNow we want to learn the error of in-sample and out-of-sample. We achieve such goal by drawing confusion matrix.\r\n\r\nFirst is in-sample, i.e. by seeing how well the algorithm is predicting training set itself.\r\n\r\n```r\r\nprediction <- predict(rffit, training)\r\nconfusionMatrix(prediction, training$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 3906    0    0    0    0\r\n##          B    0 2658    0    0    0\r\n##          C    0    0 2396    0    0\r\n##          D    0    0    0 2252    0\r\n##          E    0    0    0    0 2525\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9997, 1)\r\n##     No Information Rate : 0.2843     \r\n##     P-Value [Acc > NIR] : < 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000\r\n```\r\n\r\nThen is out-of-sample error, i.e. by seeing how well it can predict the small test part:\r\n\r\n```r\r\nprediction <- predict(rffit, testing)\r\nconfusionMatrix(prediction, testing$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1673    5    0    0    0\r\n##          B    1 1134   13    0    0\r\n##          C    0    0 1011   21    3\r\n##          D    0    0    2  942    4\r\n##          E    0    0    0    1 1075\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9915          \r\n##                  95% CI : (0.9888, 0.9937)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9893          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9994   0.9956   0.9854   0.9772   0.9935\r\n## Specificity            0.9988   0.9971   0.9951   0.9988   0.9998\r\n## Pos Pred Value         0.9970   0.9878   0.9768   0.9937   0.9991\r\n## Neg Pred Value         0.9998   0.9989   0.9969   0.9955   0.9985\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2843   0.1927   0.1718   0.1601   0.1827\r\n## Detection Prevalence   0.2851   0.1951   0.1759   0.1611   0.1828\r\n## Balanced Accuracy      0.9991   0.9963   0.9902   0.9880   0.9967\r\n```\r\nWe can see from both confusion matrices, we have pretty high accuracy for both cases.\r\n\r\n# How does it go with the testing 20 samples?\r\n\r\n```r\r\nanswer <- predict(rffit, testRaw)\r\n\r\nanswer\r\n```\r\n\r\n```\r\n##  [1] B A B A A E D B A A B C B A E E A B B B\r\n## Levels: A B C D E\r\n```\r\n\r\n```r\r\nanswer <- as.vector(answer)\r\n\r\npml_write_files = function(x) {\r\n    n = length(x)\r\n    for (i in 1:n) {\r\n        filename = paste0(\"problem_id_\", i, \".txt\")\r\n        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, \r\n            col.names = FALSE)\r\n    }\r\n}\r\n\r\npml_write_files(answer)\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}